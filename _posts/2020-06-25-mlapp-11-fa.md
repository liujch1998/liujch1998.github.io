---
layout: post
title: "Reading Notes: MLAPP Chapter 11: Latent Linear Models"
---

$$
\newcommand{\bm}[1]{\boldsymbol{#1}}
$$

这一章通过对mixture model的拓展引入了Factor Analysis (FA)，又讲了FA如何在一定假设下退化为Probabilistic PCA (PPCA)，以及顺带介绍了PCA和ICA。

# Factor Analysis (FA)

在mixture model中，latent variable $z$是一个categorical variable，因而限制了model的expressive power。FA将$z$泛化为一个服从Gaussian prior的一个vector $\bm{z} \in \mathbb{R}^L$：

$$
p(\bm{z}) = \mathcal{N} (\bm{\mu}_0, \bm{\Sigma}_0)
$$

令likelihood

$$
p(\bm{x} \mid \bm{z}, \bm{\theta}) = \mathcal{N} (\bm{W} \bm{z} + \bm{\mu}, \bm{\Psi})
$$

其中$\bm{\Psi}$应为diagonal matrix，因为$\bm{z}$应负责解释$\bm{x}$中的covariance。

FA可以理解为对Multi-Variate Normal (MVN)的一种low-rank parameterization。通过计算FA的marginal distribution可得$p(\bm{x} \mid \bm{\theta})$为一个Gaussian，其covariance matrix $\bm{\Sigma} = \bm{W} \bm{W}^T + \bm{\Psi}$。

FA会遇到unidentifiability的问题：模型的自由参数过多，导致两个参数组合生成的模型完全等价，Unidentifiability问题使得对latent variable的解读变得困难。

FA假设数据是根据一个linear manifold生成的，但很多数据其实分布在curved manifold上。考虑用piecewise linear manifold来近似curved manifold，我们得到一个mixture of FA模型。可以用EM算法来fit一个FA模型，同样也可以fit一个mixture of FA模型。

# Principal Component Analysis (PCA)

当$\bm{\Psi} = \sigma^2 \bm{I}$时，FA会退化为Probabilistic PCA (PPCA)。这个条件实质上对应了PCA中选择的reconstruction error为MSE，其中每个维度的权重均等。当$\sigma^2 \rightarrow 0$时，PPCA会进一步退化为经典的PCA：对于数据$\bm{X}$，寻找正交基$\bm{W}$和scores $\bm{Z}$使得最小化reconstruction error

$$ J = || \bm{X} - \bm{W} \bm{Z}^T || ^2_F $$

众所周知，这个问题的最优解是$\hat{\bm{W}} = \bm{V}_L$，即empirical covariance matrix $\hat{\bm{\Sigma}} = \frac{1}{N} \sum{\bm{x}_i \bm{x}_i^T} = \frac{1}{N} \bm{X}^T \bm{X}$的$L$个最大的eigenvalue所对应的eigenvector，相应的$\hat{\bm{z}}_i = \bm{W}^T \bm{x}_i$。

当$\sigma^2 > 0$时，$\bm{z}$的posterior会被prior约束，导致对$\bm{z}$的inference偏向prior mean，posterior mean不再是一个orthogonal projection。从这个角度看，$\sigma^2 \rightarrow 0$其实是创造出一个deterministic generative process，使prior的强度相对于likelihood极弱。

* PCA会保留variance最大的方向，容易受到维度间scale的影响，所以标准做法是预先对每个维度进行normalization：数据在每个维度均为zero mean和unit variance，或者等价地，我们用correlation matrix代替covariance matrix。这样也有助于表示方便，所以本文中我们假设数据已normalize。
* PCA可以通过求covariance matrix $\hat{\bm{\Sigma}}$的eigenvector来求解，还可以通过求数据$\bm{X}$的SVD来求解。通过PPCA可以推导出其EM算法，其同样适用于PCA。书中给出了一个PCA EM的形象解释，我觉得很妙：假设数据是平面上的点，principal component是一根可以围绕原点旋转的杆，数据点通过弹簧连到杆上，则E-step是将杆固定（固定上一轮的parameter estimation），允许弹簧和杆的连接点自由移动（给出latent variable的expectation）；M-step则是将连接点固定，允许杆通过旋转最小化势能（寻找更优的parameter estimation）。这个例子巧妙利用了Gaussian distribution的squared loss和弹簧的平方势能的关系，将数理统计和物理学联系到了一起。

## Independent Component Analysis (ICA)

ICA的目标是通过观察到的混合信号复原源信号。设$\bm{x}_t$为混合信号，$\bm{z}_t$为源信号，$\bm{W}$为mixing matrix，满足$\bm{z}_t = \bm{W} \bm{x}_t + \bm{\epsilon}_t$。通过inference计算$p(\bm{z}_t \mid \bm{x}_t, \theta)$可以恢复源信号。

因为PCA使用了Gaussian prior，它解出的mixing matrix和latent variables在orthogonal transformation下是不变的，所以它无法用来唯一确定源信号。ICA禁止使用Gaussian Prior，可以选择使用的有Laplace prior和Uniform prior等，以此来避免unidentifiability的问题。
---
layout: post
title: "DFTnet: efficiently training large neural networks"
---

Recently I played with neural networks, changing the matrix multiplication in NN's propagation into a convolution, with FFT to speed up computation. This architecture allows for training neural networks with larger layer sizes, given that we allow weights to be reused in a certain way. Preliminary experiments shows 93% accuracy on MNIST dataset. 

<iframe src="https://docs.google.com/gview?embedded=true&url=https://github.com/liujch1998/Lab/raw/master/ml-dft-nn/report/report.pdf" width="720" height="960" />

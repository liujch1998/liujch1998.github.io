---
layout: post
title: DFTnet (draft)
subtitle: efficiently training large neural networks
image: /img/nn.jpg
---

Recently I played with neural networks, changing the matrix multiplication in NN's propagation into a convolution, with FFT to speed up computation. This architecture allows for training neural networks with larger layer sizes, given that we allow weights to be reused in a certain way. Preliminary experiments shows 93% accuracy on MNIST dataset. 

<object data="https://raw.githubusercontent.com/liujch1998/Lab/master/ml-dft-nn/report/report.pdf" width="720" height="720" type='application/pdf' />

